{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralMachineTranslation:\n",
    "    def __init__(self, max_sequence_length=10):\n",
    "        \"\"\"\n",
    "        Initialize Neural Machine Translation Model\n",
    "        \n",
    "        Args:\n",
    "            max_sequence_length (int): Maximum length of input/output sequences\n",
    "        \"\"\"\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.input_tokenizer = None\n",
    "        self.output_tokenizer = None\n",
    "        self.model = None\n",
    "    \n",
    "    def prepare_data(self, input_texts, output_texts):\n",
    "        \"\"\"\n",
    "        Prepare input and output texts for training\n",
    "        \n",
    "        Args:\n",
    "            input_texts (list): List of input language sentences\n",
    "            output_texts (list): List of output language sentences\n",
    "        \n",
    "        Returns:\n",
    "            tuple: Tokenized and padded input and output sequences\n",
    "        \"\"\"\n",
    "        # Tokenize input texts\n",
    "        self.input_tokenizer = Tokenizer(filters='', lower=False)\n",
    "        self.input_tokenizer.fit_on_texts(input_texts)\n",
    "        input_sequences = self.input_tokenizer.texts_to_sequences(input_texts)\n",
    "        encoder_input_data = pad_sequences(input_sequences, maxlen=self.max_sequence_length, padding='post')\n",
    "        \n",
    "        # Tokenize output texts\n",
    "        self.output_tokenizer = Tokenizer(filters='', lower=False)\n",
    "        self.output_tokenizer.fit_on_texts(output_texts)\n",
    "        output_sequences = self.output_tokenizer.texts_to_sequences(output_texts)\n",
    "        decoder_input_data = pad_sequences(output_sequences, maxlen=self.max_sequence_length, padding='post')\n",
    "        \n",
    "        # One-hot encode output sequences\n",
    "        # Shift the target data by one time step\n",
    "        decoder_target_data = np.zeros((len(output_texts), self.max_sequence_length, \n",
    "                                        len(self.output_tokenizer.word_index) + 1), dtype='float32')\n",
    "        \n",
    "        for i, sequence in enumerate(decoder_input_data):\n",
    "            for t, word_index in enumerate(sequence):\n",
    "                if word_index > 0 and t > 0:\n",
    "                    decoder_target_data[i, t-1, word_index] = 1.0\n",
    "        \n",
    "        # Get vocabulary sizes\n",
    "        input_vocab_size = len(self.input_tokenizer.word_index) + 1\n",
    "        output_vocab_size = len(self.output_tokenizer.word_index) + 1\n",
    "        \n",
    "        return (encoder_input_data, decoder_input_data, decoder_target_data, \n",
    "                input_vocab_size, output_vocab_size)\n",
    "    \n",
    "    def build_model(self, input_vocab_size, output_vocab_size):\n",
    "        \"\"\"\n",
    "        Build the Neural Machine Translation model\n",
    "        \n",
    "        Args:\n",
    "            input_vocab_size (int): Size of input language vocabulary\n",
    "            output_vocab_size (int): Size of output language vocabulary\n",
    "        \"\"\"\n",
    "        # Encoder\n",
    "        encoder_inputs = tf.keras.layers.Input(shape=(self.max_sequence_length,))\n",
    "        encoder_embedding = tf.keras.layers.Embedding(\n",
    "            input_vocab_size, 256, mask_zero=True)(encoder_inputs)\n",
    "        encoder_lstm = tf.keras.layers.LSTM(256, return_sequences=True, return_state=True)\n",
    "        encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "        encoder_states = [state_h, state_c]\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_inputs = tf.keras.layers.Input(shape=(self.max_sequence_length,))\n",
    "        decoder_embedding = tf.keras.layers.Embedding(\n",
    "            output_vocab_size, 256, mask_zero=True)(decoder_inputs)\n",
    "        decoder_lstm = tf.keras.layers.LSTM(256, return_sequences=True, return_state=True)\n",
    "        decoder_outputs, _, _ = decoder_lstm(\n",
    "            decoder_embedding, initial_state=encoder_states)\n",
    "        \n",
    "        # Dense output layer\n",
    "        decoder_dense = tf.keras.layers.Dense(\n",
    "            output_vocab_size, activation='softmax')\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "        \n",
    "        # Compile the model\n",
    "        model = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "        model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        self.model = model\n",
    "    \n",
    "    def train(self, input_texts, output_texts, epochs=50, batch_size=32):\n",
    "        \"\"\"\n",
    "        Train the Neural Machine Translation model\n",
    "        \n",
    "        Args:\n",
    "            input_texts (list): Training input language sentences\n",
    "            output_texts (list): Training output language sentences\n",
    "            epochs (int): Number of training epochs\n",
    "            batch_size (int): Batch size for training\n",
    "        \"\"\"\n",
    "        # Prepare data\n",
    "        (encoder_input_data, decoder_input_data, \n",
    "         decoder_target_data, input_vocab_size, \n",
    "         output_vocab_size) = self.prepare_data(input_texts, output_texts)\n",
    "        \n",
    "        # Build model\n",
    "        self.build_model(input_vocab_size, output_vocab_size)\n",
    "        \n",
    "        # Train the model\n",
    "        self.model.fit(\n",
    "            [encoder_input_data, decoder_input_data], \n",
    "            decoder_target_data,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_split=0.2\n",
    "        )\n",
    "\n",
    "    def save(self, filepath):\n",
    "        \"\"\"\n",
    "        Save the entire model to a specified filepath\n",
    "        \"\"\"\n",
    "        self.model.save(filepath, save_format='h5')\n",
    "    \n",
    "    def translate(self, input_sentence):\n",
    "        \"\"\"\n",
    "        Translate a single sentence\n",
    "        \n",
    "        Args:\n",
    "            input_sentence (str): Sentence to translate\n",
    "        \n",
    "        Returns:\n",
    "            str: Translated sentence\n",
    "        \"\"\"\n",
    "        if not self.model or not self.input_tokenizer or not self.output_tokenizer:\n",
    "            raise ValueError(\"Model must be trained before translation\")\n",
    "        \n",
    "        # Tokenize and pad input sentence\n",
    "        input_seq = self.input_tokenizer.texts_to_sequences([input_sentence])\n",
    "        input_padded = pad_sequences(input_seq, maxlen=self.max_sequence_length, padding='post')\n",
    "        \n",
    "        # Prepare decoder input\n",
    "        decoder_input = np.zeros((1, self.max_sequence_length))\n",
    "        decoder_input[0, 0] = self.output_tokenizer.word_index.get('<start>', 1)  # Start token\n",
    "        \n",
    "        # Translation process\n",
    "        translated_words = []\n",
    "        for i in range(self.max_sequence_length):\n",
    "            # Predict next word\n",
    "            output = self.model.predict([input_padded, decoder_input])\n",
    "            \n",
    "            # Get the index of the word with the highest probability\n",
    "            predicted_word_index = np.argmax(output[0, i, :])\n",
    "            \n",
    "            # Convert index to word\n",
    "            predicted_word = self.output_tokenizer.index_word.get(predicted_word_index, '')\n",
    "            \n",
    "            # Stop if no word is predicted or we've reached max length\n",
    "            if not predicted_word or predicted_word == '<end>':\n",
    "                break\n",
    "            \n",
    "            translated_words.append(predicted_word)\n",
    "            \n",
    "            # Update decoder input\n",
    "            decoder_input[0, i+1] = predicted_word_index\n",
    "        \n",
    "        return ' '.join(translated_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts_from_file(filename):\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            # Strip whitespace and remove empty lines\n",
    "            texts = ['<start> ' + line.strip() + ' <end>' for line in file if line.strip()]\n",
    "        return texts\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {filename} not found.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {filename}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "250/250 [==============================] - 34s 124ms/step - loss: 4.6274 - accuracy: 0.2224 - val_loss: 4.2886 - val_accuracy: 0.2445\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 3.8795 - accuracy: 0.2505 - val_loss: 4.0668 - val_accuracy: 0.2585\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 33s 133ms/step - loss: 3.5954 - accuracy: 0.2593 - val_loss: 3.9785 - val_accuracy: 0.2659\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 34s 134ms/step - loss: 3.3773 - accuracy: 0.2701 - val_loss: 3.9320 - val_accuracy: 0.2714\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 36s 146ms/step - loss: 3.1734 - accuracy: 0.2836 - val_loss: 3.9185 - val_accuracy: 0.2720\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 36s 143ms/step - loss: 2.9771 - accuracy: 0.3015 - val_loss: 3.9156 - val_accuracy: 0.2787\n",
      "Epoch 7/100\n",
      "250/250 [==============================] - 35s 142ms/step - loss: 2.7955 - accuracy: 0.3226 - val_loss: 3.9073 - val_accuracy: 0.2844\n",
      "Epoch 8/100\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 2.6182 - accuracy: 0.3459 - val_loss: 3.9306 - val_accuracy: 0.2924\n",
      "Epoch 9/100\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 2.4475 - accuracy: 0.3675 - val_loss: 3.9470 - val_accuracy: 0.2945\n",
      "Epoch 10/100\n",
      "250/250 [==============================] - 34s 137ms/step - loss: 2.2877 - accuracy: 0.3906 - val_loss: 4.0161 - val_accuracy: 0.2960\n",
      "Epoch 11/100\n",
      "250/250 [==============================] - 36s 143ms/step - loss: 2.1281 - accuracy: 0.4171 - val_loss: 4.0672 - val_accuracy: 0.3013\n",
      "Epoch 12/100\n",
      "250/250 [==============================] - 35s 138ms/step - loss: 1.9731 - accuracy: 0.4419 - val_loss: 4.1199 - val_accuracy: 0.3023\n",
      "Epoch 13/100\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 1.8216 - accuracy: 0.4666 - val_loss: 4.2541 - val_accuracy: 0.3083\n",
      "Epoch 14/100\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 1.6777 - accuracy: 0.4920 - val_loss: 4.3793 - val_accuracy: 0.3056\n",
      "Epoch 15/100\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 1.5364 - accuracy: 0.5158 - val_loss: 4.4407 - val_accuracy: 0.3047\n",
      "Epoch 16/100\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 1.4005 - accuracy: 0.5368 - val_loss: 4.6231 - val_accuracy: 0.3039\n",
      "Epoch 17/100\n",
      "250/250 [==============================] - 34s 137ms/step - loss: 1.2715 - accuracy: 0.5584 - val_loss: 4.6915 - val_accuracy: 0.3080\n",
      "Epoch 18/100\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 1.1428 - accuracy: 0.5794 - val_loss: 4.8345 - val_accuracy: 0.2997\n",
      "Epoch 19/100\n",
      "250/250 [==============================] - 34s 137ms/step - loss: 1.0149 - accuracy: 0.5987 - val_loss: 5.0510 - val_accuracy: 0.3012\n",
      "Epoch 20/100\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 0.9032 - accuracy: 0.6169 - val_loss: 5.2105 - val_accuracy: 0.3050\n",
      "Epoch 21/100\n",
      "250/250 [==============================] - 35s 138ms/step - loss: 0.7917 - accuracy: 0.6359 - val_loss: 5.3736 - val_accuracy: 0.2995\n",
      "Epoch 22/100\n",
      "250/250 [==============================] - 34s 138ms/step - loss: 0.6970 - accuracy: 0.6534 - val_loss: 5.4454 - val_accuracy: 0.3061\n",
      "Epoch 23/100\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 0.6084 - accuracy: 0.6711 - val_loss: 5.6496 - val_accuracy: 0.3031\n",
      "Epoch 24/100\n",
      "250/250 [==============================] - 35s 139ms/step - loss: 0.5303 - accuracy: 0.6840 - val_loss: 5.8133 - val_accuracy: 0.2925\n",
      "Epoch 25/100\n",
      "250/250 [==============================] - 34s 134ms/step - loss: 0.4569 - accuracy: 0.6998 - val_loss: 5.8833 - val_accuracy: 0.3042\n",
      "Epoch 26/100\n",
      "250/250 [==============================] - 34s 137ms/step - loss: 0.3989 - accuracy: 0.7120 - val_loss: 6.0030 - val_accuracy: 0.3032\n",
      "Epoch 27/100\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 0.3472 - accuracy: 0.7229 - val_loss: 6.1868 - val_accuracy: 0.2979\n",
      "Epoch 28/100\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 0.2988 - accuracy: 0.7331 - val_loss: 6.5353 - val_accuracy: 0.2935\n",
      "Epoch 29/100\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 0.2635 - accuracy: 0.7388 - val_loss: 6.6399 - val_accuracy: 0.2874\n",
      "Epoch 30/100\n",
      "250/250 [==============================] - 33s 133ms/step - loss: 0.2325 - accuracy: 0.7456 - val_loss: 6.7940 - val_accuracy: 0.2986\n",
      "Epoch 31/100\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 0.2001 - accuracy: 0.7523 - val_loss: 6.9435 - val_accuracy: 0.2883\n",
      "Epoch 32/100\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 0.1764 - accuracy: 0.7570 - val_loss: 7.0992 - val_accuracy: 0.2876\n",
      "Epoch 33/100\n",
      "250/250 [==============================] - 35s 139ms/step - loss: 0.1564 - accuracy: 0.7614 - val_loss: 7.2260 - val_accuracy: 0.2854\n",
      "Epoch 34/100\n",
      "250/250 [==============================] - 38s 154ms/step - loss: 0.1378 - accuracy: 0.7644 - val_loss: 7.3340 - val_accuracy: 0.2874\n",
      "Epoch 35/100\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 0.1224 - accuracy: 0.7684 - val_loss: 7.5126 - val_accuracy: 0.2915\n",
      "Epoch 36/100\n",
      "250/250 [==============================] - 33s 133ms/step - loss: 0.1101 - accuracy: 0.7720 - val_loss: 7.7124 - val_accuracy: 0.2724\n",
      "Epoch 37/100\n",
      "250/250 [==============================] - 35s 139ms/step - loss: 0.0973 - accuracy: 0.7746 - val_loss: 7.7639 - val_accuracy: 0.2918\n",
      "Epoch 38/100\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 0.0885 - accuracy: 0.7763 - val_loss: 8.0516 - val_accuracy: 0.2766\n",
      "Epoch 39/100\n",
      "250/250 [==============================] - 34s 137ms/step - loss: 0.0791 - accuracy: 0.7793 - val_loss: 8.0760 - val_accuracy: 0.2790\n",
      "Epoch 40/100\n",
      "250/250 [==============================] - 34s 137ms/step - loss: 0.0732 - accuracy: 0.7809 - val_loss: 8.3037 - val_accuracy: 0.2830\n",
      "Epoch 41/100\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 0.0672 - accuracy: 0.7822 - val_loss: 8.2493 - val_accuracy: 0.2811\n",
      "Epoch 42/100\n",
      "250/250 [==============================] - 34s 137ms/step - loss: 0.0627 - accuracy: 0.7837 - val_loss: 8.3661 - val_accuracy: 0.2801\n",
      "Epoch 43/100\n",
      "250/250 [==============================] - 34s 137ms/step - loss: 0.0599 - accuracy: 0.7844 - val_loss: 8.4688 - val_accuracy: 0.2814\n",
      "Epoch 44/100\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 0.0561 - accuracy: 0.7842 - val_loss: 8.6668 - val_accuracy: 0.2821\n",
      "Epoch 45/100\n",
      "250/250 [==============================] - 34s 137ms/step - loss: 0.0523 - accuracy: 0.7865 - val_loss: 8.6610 - val_accuracy: 0.2802\n",
      "Epoch 46/100\n",
      "250/250 [==============================] - 35s 142ms/step - loss: 0.0501 - accuracy: 0.7870 - val_loss: 8.8423 - val_accuracy: 0.2751\n",
      "Epoch 47/100\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 0.0486 - accuracy: 0.7869 - val_loss: 8.9671 - val_accuracy: 0.2789\n",
      "Epoch 48/100\n",
      "250/250 [==============================] - 34s 134ms/step - loss: 0.0471 - accuracy: 0.7869 - val_loss: 9.0445 - val_accuracy: 0.2767\n",
      "Epoch 49/100\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 0.0468 - accuracy: 0.7871 - val_loss: 9.1029 - val_accuracy: 0.2848\n",
      "Epoch 50/100\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 0.0452 - accuracy: 0.7877 - val_loss: 9.2758 - val_accuracy: 0.2701\n",
      "Epoch 51/100\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 0.0455 - accuracy: 0.7879 - val_loss: 9.2545 - val_accuracy: 0.2760\n",
      "Epoch 52/100\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 0.0428 - accuracy: 0.7878 - val_loss: 9.3694 - val_accuracy: 0.2695\n",
      "Epoch 53/100\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 0.0423 - accuracy: 0.7889 - val_loss: 9.3624 - val_accuracy: 0.2687\n",
      "Epoch 54/100\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 0.0404 - accuracy: 0.7893 - val_loss: 9.5169 - val_accuracy: 0.2668\n",
      "Epoch 55/100\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 0.0417 - accuracy: 0.7888 - val_loss: 9.4688 - val_accuracy: 0.2776\n",
      "Epoch 56/100\n",
      "250/250 [==============================] - 194s 780ms/step - loss: 0.0389 - accuracy: 0.7890 - val_loss: 9.7489 - val_accuracy: 0.2621\n",
      "Epoch 57/100\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 0.0396 - accuracy: 0.7886 - val_loss: 9.6973 - val_accuracy: 0.2707\n",
      "Epoch 58/100\n",
      "250/250 [==============================] - 41s 164ms/step - loss: 0.0386 - accuracy: 0.7894 - val_loss: 9.7906 - val_accuracy: 0.2656\n",
      "Epoch 59/100\n",
      "250/250 [==============================] - 37s 149ms/step - loss: 0.0388 - accuracy: 0.7890 - val_loss: 9.7635 - val_accuracy: 0.2668\n",
      "Epoch 60/100\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 0.0369 - accuracy: 0.7894 - val_loss: 10.0582 - val_accuracy: 0.2643\n",
      "Epoch 61/100\n",
      "250/250 [==============================] - 41s 163ms/step - loss: 0.0370 - accuracy: 0.7894 - val_loss: 10.1023 - val_accuracy: 0.2645\n",
      "Epoch 62/100\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 0.0374 - accuracy: 0.7893 - val_loss: 10.1804 - val_accuracy: 0.2593\n",
      "Epoch 63/100\n",
      "250/250 [==============================] - 34s 137ms/step - loss: 0.0368 - accuracy: 0.7892 - val_loss: 10.2591 - val_accuracy: 0.2561\n",
      "Epoch 64/100\n",
      "250/250 [==============================] - 37s 150ms/step - loss: 0.0375 - accuracy: 0.7893 - val_loss: 10.4930 - val_accuracy: 0.2508\n",
      "Epoch 65/100\n",
      "250/250 [==============================] - 35s 142ms/step - loss: 0.0346 - accuracy: 0.7894 - val_loss: 10.3183 - val_accuracy: 0.2640\n",
      "Epoch 66/100\n",
      "250/250 [==============================] - 34s 137ms/step - loss: 0.0348 - accuracy: 0.7901 - val_loss: 10.4483 - val_accuracy: 0.2566\n",
      "Epoch 67/100\n",
      "250/250 [==============================] - 35s 139ms/step - loss: 0.0343 - accuracy: 0.7896 - val_loss: 10.9145 - val_accuracy: 0.2486\n",
      "Epoch 68/100\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 0.0351 - accuracy: 0.7897 - val_loss: 10.6952 - val_accuracy: 0.2657\n",
      "Epoch 69/100\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 0.0330 - accuracy: 0.7902 - val_loss: 10.5867 - val_accuracy: 0.2604\n",
      "Epoch 70/100\n",
      "250/250 [==============================] - 34s 138ms/step - loss: 0.0335 - accuracy: 0.7900 - val_loss: 10.7439 - val_accuracy: 0.2574\n",
      "Epoch 71/100\n",
      "250/250 [==============================] - 34s 137ms/step - loss: 0.0347 - accuracy: 0.7899 - val_loss: 10.8382 - val_accuracy: 0.2543\n",
      "Epoch 72/100\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 0.0321 - accuracy: 0.7904 - val_loss: 10.8964 - val_accuracy: 0.2553\n",
      "Epoch 73/100\n",
      "250/250 [==============================] - 35s 140ms/step - loss: 0.0321 - accuracy: 0.7905 - val_loss: 10.9514 - val_accuracy: 0.2528\n",
      "Epoch 74/100\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 0.0322 - accuracy: 0.7908 - val_loss: 10.9836 - val_accuracy: 0.2551\n",
      "Epoch 75/100\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 0.0331 - accuracy: 0.7901 - val_loss: 10.9500 - val_accuracy: 0.2581\n",
      "Epoch 76/100\n",
      "250/250 [==============================] - 51s 205ms/step - loss: 0.0325 - accuracy: 0.7895 - val_loss: 11.2334 - val_accuracy: 0.2530\n",
      "Epoch 77/100\n",
      "250/250 [==============================] - 43s 172ms/step - loss: 0.0325 - accuracy: 0.7895 - val_loss: 11.1198 - val_accuracy: 0.2561\n",
      "Epoch 78/100\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 0.0317 - accuracy: 0.7905 - val_loss: 11.4775 - val_accuracy: 0.2474\n",
      "Epoch 79/100\n",
      "250/250 [==============================] - 41s 164ms/step - loss: 0.0321 - accuracy: 0.7903 - val_loss: 11.4933 - val_accuracy: 0.2467\n",
      "Epoch 80/100\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 0.0326 - accuracy: 0.7900 - val_loss: 11.4663 - val_accuracy: 0.2496\n",
      "Epoch 81/100\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 0.0310 - accuracy: 0.7904 - val_loss: 11.6272 - val_accuracy: 0.2495\n",
      "Epoch 82/100\n",
      "250/250 [==============================] - 52s 208ms/step - loss: 0.0310 - accuracy: 0.7898 - val_loss: 11.6422 - val_accuracy: 0.2538\n",
      "Epoch 83/100\n",
      "250/250 [==============================] - 49s 198ms/step - loss: 0.0314 - accuracy: 0.7904 - val_loss: 11.6950 - val_accuracy: 0.2549\n",
      "Epoch 84/100\n",
      "250/250 [==============================] - 52s 207ms/step - loss: 0.0315 - accuracy: 0.7901 - val_loss: 11.6695 - val_accuracy: 0.2516\n",
      "Epoch 85/100\n",
      "250/250 [==============================] - 50s 199ms/step - loss: 0.0317 - accuracy: 0.7902 - val_loss: 11.7761 - val_accuracy: 0.2501\n",
      "Epoch 86/100\n",
      "250/250 [==============================] - 35s 139ms/step - loss: 0.0310 - accuracy: 0.7902 - val_loss: 12.2509 - val_accuracy: 0.2422\n",
      "Epoch 87/100\n",
      "250/250 [==============================] - 34s 138ms/step - loss: 0.0303 - accuracy: 0.7903 - val_loss: 12.2176 - val_accuracy: 0.2459\n",
      "Epoch 88/100\n",
      "250/250 [==============================] - 39s 158ms/step - loss: 0.0294 - accuracy: 0.7906 - val_loss: 12.4887 - val_accuracy: 0.2428\n",
      "Epoch 89/100\n",
      "250/250 [==============================] - 36s 146ms/step - loss: 0.0296 - accuracy: 0.7902 - val_loss: 12.1879 - val_accuracy: 0.2488\n",
      "Epoch 90/100\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 0.0288 - accuracy: 0.7908 - val_loss: 12.1685 - val_accuracy: 0.2537\n",
      "Epoch 91/100\n",
      "250/250 [==============================] - 38s 151ms/step - loss: 0.0298 - accuracy: 0.7902 - val_loss: 12.5344 - val_accuracy: 0.2442\n",
      "Epoch 92/100\n",
      "250/250 [==============================] - 44s 178ms/step - loss: 0.0317 - accuracy: 0.7897 - val_loss: 12.4018 - val_accuracy: 0.2461\n",
      "Epoch 93/100\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 0.0296 - accuracy: 0.7901 - val_loss: 12.3588 - val_accuracy: 0.2493\n",
      "Epoch 94/100\n",
      "250/250 [==============================] - 49s 195ms/step - loss: 0.0296 - accuracy: 0.7903 - val_loss: 12.4560 - val_accuracy: 0.2477\n",
      "Epoch 95/100\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 0.0287 - accuracy: 0.7899 - val_loss: 12.6726 - val_accuracy: 0.2488\n",
      "Epoch 96/100\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 0.0294 - accuracy: 0.7904 - val_loss: 12.7466 - val_accuracy: 0.2445\n",
      "Epoch 97/100\n",
      "250/250 [==============================] - 35s 138ms/step - loss: 0.0296 - accuracy: 0.7908 - val_loss: 12.9663 - val_accuracy: 0.2413\n",
      "Epoch 98/100\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 0.0282 - accuracy: 0.7910 - val_loss: 13.0499 - val_accuracy: 0.2463\n",
      "Epoch 99/100\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 0.0297 - accuracy: 0.7905 - val_loss: 13.0234 - val_accuracy: 0.2448\n",
      "Epoch 100/100\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 0.0292 - accuracy: 0.7904 - val_loss: 13.3583 - val_accuracy: 0.2381\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Input: When he asked who had broken the window, all the boys put on an air of innocence.\n",
      "Translation: J'aime ses films.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    model_save_path = 'en_fr_model.h5'\n",
    "\n",
    "    \n",
    "    input_texts = read_texts_from_file('datasets/eng_fr.en')\n",
    "    output_texts = read_texts_from_file('datasets/french')\n",
    "\n",
    "    input_texts = input_texts[:10000]\n",
    "    output_texts = output_texts[:10000]\n",
    "    \n",
    "    # Initialize and train the model\n",
    "    nmt_model = NeuralMachineTranslation(max_sequence_length=5)\n",
    "    \n",
    "    # Train the model\n",
    "    nmt_model.train(input_texts, output_texts, epochs=100)\n",
    "    nmt_model.save(model_save_path)\n",
    "\n",
    "    with open(\"en_fr_input_tokenizer.pkl\", \"wb\") as f:\n",
    "        pickle.dump(nmt_model.input_tokenizer, f)\n",
    "\n",
    "    with open(\"en_fr_output_tokenizer.pkl\", \"wb\") as f:\n",
    "        pickle.dump(nmt_model.output_tokenizer, f)\n",
    "\n",
    "\n",
    "    \n",
    "    # Translate the first sentence\n",
    "    if input_texts:\n",
    "        input_sentence = input_texts[0].replace('<start>', '').replace('<end>', '').strip()\n",
    "        translation = nmt_model.translate(input_sentence)\n",
    "        print(f\"Input: {input_sentence}\")\n",
    "        print(f\"Translation: {translation}\")\n",
    "    else:\n",
    "        print(\"No input texts available for translation.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nullclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
