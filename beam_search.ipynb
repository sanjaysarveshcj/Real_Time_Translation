{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import logging\n",
    "from typing import List, Tuple\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearchTranslator:\n",
    "    def __init__(self, \n",
    "                 model_path: str, \n",
    "                 english_tokenizer_path: str, \n",
    "                 french_tokenizer_path: str, \n",
    "                 max_length_path: str, \n",
    "                 beam_width: int = 3):\n",
    "        \"\"\"\n",
    "        Initialize the beam search translator with robust file loading.\n",
    "        \"\"\"\n",
    "        # Set up logging\n",
    "        logging.basicConfig(level=logging.DEBUG)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Validate file paths\n",
    "        self._validate_file_paths(model_path, english_tokenizer_path, \n",
    "                                  french_tokenizer_path, max_length_path)\n",
    "        \n",
    "        # Load model\n",
    "        try:\n",
    "            self.model = load_model(model_path)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to load model: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Load tokenizers\n",
    "        self.english_tokenizer = self._load_tokenizer(english_tokenizer_path)\n",
    "        self.french_tokenizer = self._load_tokenizer(french_tokenizer_path)\n",
    "\n",
    "        # Load max length\n",
    "        with open(max_length_path, 'r') as f:\n",
    "            self.max_length = json.load(f)\n",
    "\n",
    "        # Create reverse mapping for tokenizers\n",
    "        self.index_to_english = {id: word for word, id in self.english_tokenizer.word_index.items()}\n",
    "        self.index_to_french = {id: word for word, id in self.french_tokenizer.word_index.items()}\n",
    "        self.index_to_english[0] = '<PAD>'\n",
    "        self.index_to_french[0] = '<PAD>'\n",
    "        \n",
    "        self.beam_width = beam_width\n",
    "\n",
    "    def _validate_file_paths(self, *paths):\n",
    "        \"\"\"Validate that all provided file paths exist.\"\"\"\n",
    "        for path in paths:\n",
    "            if not os.path.exists(path):\n",
    "                raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    def _load_tokenizer(self, tokenizer_path: str):\n",
    "        \"\"\"Load tokenizer from JSON file.\"\"\"\n",
    "        try:\n",
    "            with open(tokenizer_path, 'r') as f:\n",
    "                tokenizer = tokenizer_from_json(json.load(f))\n",
    "                self.logger.debug(f\"Tokenizer word index size: {len(tokenizer.word_index)}\")\n",
    "                return tokenizer\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to load tokenizer: {e}\")\n",
    "            raise\n",
    "\n",
    "    def preprocess_text(self, sentence: str) -> str:\n",
    "        \"\"\"Preprocess input sentence.\"\"\"\n",
    "        sentence = re.sub(r'[^\\w\\s]', '', sentence).lower()\n",
    "        return sentence\n",
    "\n",
    "    def beam_search_translate(self, input_seq: np.ndarray) -> List[int]:\n",
    "        \"\"\"Perform beam search decoding with robust error handling.\"\"\"\n",
    "        beam_candidates = [([], 0.0)]\n",
    "        \n",
    "        for _ in range(self.max_length):\n",
    "            next_candidates = []\n",
    "            \n",
    "            for candidate, score in beam_candidates:\n",
    "                # Prepare decoder input\n",
    "                decoder_input = np.zeros((1, self.max_length), dtype=int)\n",
    "                decoder_input[0, :len(candidate)] = candidate\n",
    "                \n",
    "                try:\n",
    "                    # Predict next token probabilities\n",
    "                    prediction = self.model.predict(input_seq)[0]\n",
    "                    logits = prediction[len(candidate)]\n",
    "                    \n",
    "                    # Get top beam_width candidates\n",
    "                    top_indices = np.argsort(logits)[-self.beam_width:]\n",
    "                    \n",
    "                    for idx in top_indices:\n",
    "                        new_candidate = candidate + [idx]\n",
    "                        new_score = score + np.log(logits[idx] + 1e-10)\n",
    "                        next_candidates.append((new_candidate, new_score))\n",
    "                \n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Prediction error: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Sort and select top candidates\n",
    "            next_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "            beam_candidates = next_candidates[:self.beam_width]\n",
    "            \n",
    "            # Stop if all candidates end or reach max length\n",
    "            if all(len(c[0]) >= self.max_length for c in beam_candidates):\n",
    "                break\n",
    "        \n",
    "        # Return best candidate\n",
    "        best_candidate = max(beam_candidates, key=lambda x: x[1])[0]\n",
    "        return best_candidate\n",
    "\n",
    "    def translate_sentence(self, english_sentence: str) -> str:\n",
    "        \"\"\"Translate English sentence to French.\"\"\"\n",
    "        # Preprocess and tokenize\n",
    "        preprocessed = self.preprocess_text(english_sentence)\n",
    "        input_seq = self.english_tokenizer.texts_to_sequences([preprocessed])\n",
    "        \n",
    "        # Pad sequence\n",
    "        input_seq = pad_sequences(input_seq, maxlen=self.max_length, padding='post')\n",
    "        \n",
    "        # Beam search translation\n",
    "        translated_indices = self.beam_search_translate(input_seq)\n",
    "        \n",
    "        # Convert indices to words\n",
    "        translation = ' '.join([self.index_to_french.get(idx, '<UNK>') for idx in translated_indices])\n",
    "        \n",
    "        return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:__main__:Tokenizer word index size: 199\n",
      "DEBUG:__main__:Tokenizer word index size: 344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 393ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "\n",
      "English: new jersey is sometimes quiet during autumn\n",
      "\n",
      "French:  new jersey est parfois calme à l' automne <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct: le new jersey est parfois calme en automne\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    MODEL_PATH = 'english_to_french_model'\n",
    "    ENGLISH_TOKENIZER_PATH = 'english_tokenizer.json'\n",
    "    FRENCH_TOKENIZER_PATH = 'french_tokenizer.json'\n",
    "    MAX_LENGTH_PATH = 'sequence_length.json'\n",
    "\n",
    "    try:\n",
    "        translator = BeamSearchTranslator(\n",
    "            model_path=MODEL_PATH,\n",
    "            english_tokenizer_path=ENGLISH_TOKENIZER_PATH,\n",
    "            french_tokenizer_path=FRENCH_TOKENIZER_PATH,\n",
    "            max_length_path=MAX_LENGTH_PATH,\n",
    "            beam_width=3\n",
    "        )\n",
    "\n",
    "        sentences = [\n",
    "            \"new jersey is sometimes quiet during autumn\",\n",
    "        ]\n",
    "\n",
    "        for sentence in sentences:\n",
    "            french_translation = translator.translate_sentence(sentence)\n",
    "            print(f\"\\nEnglish: {sentence}\\n\")\n",
    "            print(f\"French:  {french_translation}\\n\")\n",
    "            print(\"Correct: le new jersey est parfois calme en automne\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Translation failed: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nullclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
